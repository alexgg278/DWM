{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWM (Dynamic Weighted Majority)\n",
    "\n",
    "In this notebook the algorithm DWM for concept drift handleling is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGGER Concepts\n",
    "\n",
    "The STAGGER concepts constitute a way of creating a streaming dataset to evaluate the response of an algortihm to concept drift.\n",
    "\n",
    "It includes three attributes with 3 values per asttribute. The strategy is to label as True or False different combinations of attributes in sucessive time slots to evaluate the performance.\n",
    "\n",
    "* **Attributes:**\n",
    "    - ***Color:*** Green, blue and red.\n",
    "    - ***Shape:*** Triangle, circle and rectangle.\n",
    "    - ***Size:*** Small, medium and large.\n",
    "    \n",
    "The presentation of training examples lasts for 120 time steps, and at each time step, the learner receives one example. For the first 40 time steps, the target concept is color = red ∧ size = small. During the next 40 time steps, the target concept is color = green ∨ shape = circle. Finally, during the last 40 time steps, the target concept is size = medium ∨ size = large. A visualization of these concepts.\n",
    "\n",
    "To evaluate the learner, at each time step, one randomly generates 100 examples of the current target concept, presents these to the performance element, and computes the percent correctly predicted. In our experiments, we repeated this procedure 50 times and averaged the accuracies over these runs. We also computed 95% confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![title](stagger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwm Implementation\n",
    "\n",
    "\n",
    "* **Concept Drift:** It is the phenomena in which the label of data instances shifts over time, i.e., two similar samples will have different label depending on the time.\n",
    "\n",
    "* **Ensemble Methods:** It is the method in which several learnes are used to train different classifiers to later predict the class label based on a voting or weighted approach.\n",
    "\n",
    "    * *Bagging*\n",
    "    * *Boosting*\n",
    "    * *Stacking*\n",
    "    \n",
    "**<font color=blue>DWM algorithm</font>**\n",
    "\n",
    "* ***Description:***\n",
    "\n",
    "    - The formal algorithm for DWM appears in Figure 1. The algorithm maintains a set of m experts, E, each with a weight, wi for i = 1,...,m. Input to the algorithm is n training examples, each consisting of a feature vector and a class label. The parameters also include the number of classes(c) and β, a multiplicative factor that DWM uses to decrease an expert’s weight when it predicts incorrectly. A typical value for β is 0.5. The parameter θ is a threshold for removing poorly performing experts. If an expert’s weight falls below this threshold, then DWM removes it from the ensemble. Finally, the parameter p determines how often DWM creates and removes experts. We found this parameter useful and necessary for large or noisy problems, which we discuss further inSection 4.2. In the following discussion, we assume p = 1. \n",
    "    \n",
    "    - DWM begins by creating an ensemble containing a single learner with a weight of one (lines 1–3 of Figure 1). Initially, this learner could predict a default class, or it could predict using previous experience, background knowledge, or both. DWM then takes a single example (or perhaps a set ofexamples) from the stream and presents it to the single learner to classify (line 7). If the learner’s prediction is wrong (line 8), then DWM decreases the learner’s weight by multiplying it by β (line 9). Since there is one expert in the ensemble, its prediction is DWM’s global prediction (lines 12 and 24). \n",
    "    \n",
    "    - If DWM’s global prediction is incorrect (line 16), then it creates a new learner with a weight of one (lines 17–19). DWM then trains the experts in the ensemble on the new example (line 23). After training, DWM outputs its global prediction (line 24). When there are multiple learners, DWM obtains a classification from each member of the ensemble (lines 6 and 7). If one’s prediction is incorrect, then DWM decreases its weight (lines 8 and 9). Regardless of the correctness of the prediction, DWM uses each learner’s prediction and its weight to compute a weighted sum for each class (line 10). The class with the most weight is set as the global prediction (line 12). \n",
    "    \n",
    "    - Since DWM always decreases the weights of experts, it normalizes the weights by scaling them uniformly so that, after the transformation, the maximum weight is one (line 14). This prevents newly added experts from dominating predictions. DWM also removes poorly performing experts by removing those with a weight less than the threshold θ (line 15), although it will not remove the last expert in the ensemble. As mentioned previously, if the global prediction is incorrect (line 16), DWM adds a new expert to the ensemble with a weight of one (lines 17–19). Finally, after using the new example to train each learner in the ensemble (lines 22 and 23), DWM outputs the global prediction, which is the weighted vote of the expert predictions (line 24).\n",
    "    \n",
    "    - As mentioned previously, the parameter p lets DWM better cope with many or noisy examples. p defines the period over which DWM will not update learners’ weights (line 8) and will not remove or create experts (line 13). During this period, however, DWM still trains the learners (lines 22 and 23).\n",
    "    \n",
    "    - DWM is a general algorithm for coping with concept drift. One can use any online learning algorithm as the base learner. To date, we have evaluated two such algorithms, naive Bayes and Incremental Tree Inducer, and we describe these versions in the next two sections.\n",
    "\n",
    " \n",
    "* ***Elements:***\n",
    "\n",
    "    - m: Number of experts\n",
    "    - E: Expert\n",
    "    - wj: Expert's weight\n",
    "    - n: Input training samples\n",
    "    - c: number of classes\n",
    "    - β: decreasing weight factor (0 ≤ β < 1)\n",
    "    - θ: Removing learner weight threshold\n",
    "    - p: How often DWM removes or update weights\n",
    "    - Λ,λ ∈ {1,..., c}: global and local predictions\n",
    "\n",
    "\n",
    "* ***Algorithm:***\n",
    "\n",
    "    / 1. m ← 1\n",
    "\n",
    "    / 2. em ← Create-New-Expert()\n",
    "\n",
    "    / 3. wm ← 1\n",
    "\n",
    "    / 4. for i ← 1,...,n // Loop over examples\n",
    "\n",
    "        / 5. ~σ ← 0\n",
    "\n",
    "        / 6. for j ← 1,...,m // Loop over experts\n",
    "\n",
    "            / 7. λ ← Classify(e j,~xi)\n",
    "\n",
    "            / 8. if (λ 6= yi and i mod p = 0)\n",
    "\n",
    "                / 9. wj ← βwj\n",
    "\n",
    "            / 10. σλ ← σλ +wj\n",
    "\n",
    "        / 11. end;\n",
    "\n",
    "        / 12. Λ ← argmaxj σj\n",
    "\n",
    "        / 13. if (i mod p = 0)\n",
    "\n",
    "            / 14. w ← Normalize-Weights(w)\n",
    "\n",
    "            / 15. {e,w} ← Remove-Experts({e,w},θ)\n",
    "\n",
    "            / 16. if (Λ 6= yi)\n",
    "\n",
    "                / 17. m ← m+1\n",
    "\n",
    "                / 18. em ← Create-New-Expert()\n",
    "\n",
    "                / 19. wm ← 1\n",
    "\n",
    "            / 20. end;\n",
    "\n",
    "        / 21. end;\n",
    "\n",
    "        / 22. for j ← 1,...,m\n",
    "\n",
    "            / 23. e j ← Train(e j,~xi, yi)\n",
    "\n",
    "            / 24. output Λ\n",
    "\n",
    "        end;\n",
    "\n",
    "    end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
